import os
import time
import json
import asyncio
import gradio as gr

# set the env
from dotenv import load_dotenv
load_dotenv()

# get the root path of the project
current_file_path = os.path.dirname(os.path.abspath(__file__))
root_path = os.path.abspath(current_file_path)

from textwrap import dedent
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate

class OurLLM:
    def __init__(self, model="gpt-4o"):
        '''
        params: 
            model: str, 
                æ¨¡å‹åç§° ["GLM-4-Flash", "GLM-4V-Flash", 
                         "gpt-4o-mini", "gpt-4o", "o1-mini", 
                         "gemini-1.5-flash-002", "gemini-1.5-pro-002",
                         "Qwen/Qwen2.5-7B-Instruct", "Qwen/Qwen2.5-Coder-7B-Instruct"]
        '''

        self.model_name = model

        OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
        OPENAI_API_KEY_DF = os.getenv('OPENAI_API_KEY_DF', OPENAI_API_KEY)
        OPENAI_API_KEY_AZ = os.getenv('OPENAI_API_KEY_AZ', OPENAI_API_KEY)
        OPENAI_API_KEY_CD = os.getenv('OPENAI_API_KEY_CD')
        OPENAI_API_KEY_O1 = os.getenv('OPENAI_API_KEY_O1')
        OPENAI_API_KEY_GLM = os.getenv('OPENAI_API_KEY_GLM')
        OPENAI_API_KEY_SC = os.getenv('OPENAI_API_KEY_SC')

        OPENAI_BASE_URL = os.getenv('OPENAI_BASE_URL')
        OPENAI_BASE_URL_GLM = os.getenv('OPENAI_BASE_URL_GLM')
        OPENAI_BASE_URL_SC = os.getenv('OPENAI_BASE_URL_SC')

        # åˆ›å»º API Key æ˜ å°„
        apiKeyMap = {
            'gemini': {"base_url": OPENAI_BASE_URL, "api_key": OPENAI_API_KEY_DF},
            'gpt': {"base_url": OPENAI_BASE_URL, "api_key": OPENAI_API_KEY_AZ},
            'o1': {"base_url": OPENAI_BASE_URL, "api_key": OPENAI_API_KEY_O1},
            'claude': {"base_url": OPENAI_BASE_URL, "api_key": OPENAI_API_KEY_CD},
            'glm': {"base_url": OPENAI_BASE_URL_GLM, "api_key": OPENAI_API_KEY_GLM},
            'qwen': {"base_url": OPENAI_BASE_URL_SC, "api_key": OPENAI_API_KEY_SC},
        }

        for name, info in apiKeyMap.items():
            if name in model.lower():
                self.base_url = info["base_url"]
                self.api_key = info["api_key"]
                break
        assert self.base_url is not None, f"Base URL not found for model: {model}"
        assert self.api_key is not None, f"API key not found for model: {model}"

        chat_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", "{system_prompt}"),
                ("human", "{input}"),
                # ("ai", "{chat_history}"),
            ]
        )
        self.chat_prompt = chat_prompt
        self.llm = self.get_llm(model)

    def clean_json(self, s):
        return s.replace("```json", "").replace("```", "").strip()

    def get_system_prompt(self, mode="assistant"):
        prompt_map = {
            "assistant": dedent("""
                ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œæ“…é•¿ç”¨ç®€æ´çš„ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
                è¯·ç¡®ä¿ä½ çš„å›ç­”å‡†ç¡®ã€æ¸…æ™°ã€æœ‰æ¡ç†ï¼Œå¹¶ä¸”ç¬¦åˆä¸­æ–‡çš„è¯­è¨€ä¹ æƒ¯ã€‚
                é‡è¦æç¤ºï¼š
                1. å›ç­”è¦ç®€æ´æ˜äº†ï¼Œé¿å…å†—é•¿
                2. ä½¿ç”¨é€‚å½“çš„ä¸“ä¸šæœ¯è¯­
                3. ä¿æŒå®¢è§‚ä¸­ç«‹çš„è¯­æ°”
                4. å¦‚æœä¸ç¡®å®šï¼Œè¦æ˜ç¡®æŒ‡å‡º
            """),
            # search
            "keyword_expand": dedent("""
                ä½ æ˜¯ä¸€ä¸ªæœç´¢å…³é”®è¯æ‰©å±•ä¸“å®¶ï¼Œæ“…é•¿å°†ç”¨æˆ·çš„æœç´¢æ„å›¾è½¬åŒ–ä¸ºå¤šä¸ªç›¸å…³çš„æœç´¢è¯æˆ–çŸ­è¯­ã€‚
                ç”¨æˆ·ä¼šè¾“å…¥ä¸€æ®µæè¿°ä»–ä»¬æœç´¢éœ€æ±‚çš„æ–‡æœ¬ï¼Œè¯·ä½ ç”Ÿæˆä¸ä¹‹ç›¸å…³çš„å…³é”®è¯åˆ—è¡¨ã€‚
                ä½ éœ€è¦è¿”å›ä¸€ä¸ªå¯ä»¥ç›´æ¥è¢« json åº“è§£æçš„å“åº”ï¼ŒåŒ…å«ä»¥ä¸‹å†…å®¹ï¼š
                {
                    "keywords": [å…³é”®è¯åˆ—è¡¨],
                }
                é‡è¦æç¤ºï¼š
                1. å…³é”®è¯åº”è¯¥åŒ…å«åŒä¹‰è¯ã€è¿‘ä¹‰è¯ã€ä¸Šä½è¯ã€ä¸‹ä½è¯
                2. çŸ­è¯­è¦ä½“ç°ä¸åŒçš„è¡¨è¾¾æ–¹å¼å’Œç»„åˆ
                3. æè¿°å¥å­è¦æ¶µç›–ä¸åŒçš„åº”ç”¨åœºæ™¯å’Œç”¨é€”
                4. æ‰€æœ‰å†…å®¹å¿…é¡»ä¸åŸå§‹æœç´¢æ„å›¾é«˜åº¦ç›¸å…³
                5. æ‰©å±•æœç´¢æ„å›¾åˆ°ç›¸å…³çš„åº”ç”¨åœºæ™¯å’Œå·¥å…·ï¼Œä¾‹å¦‚:
                    - å¦‚æœæœç´¢"PDFè½¬MD"ï¼Œåº”åŒ…å«PDFå†…å®¹æå–ã€PDFè§£æå·¥å…·ã€PDFæ•°æ®å¤„ç†ç­‰
                    - å¦‚æœæœç´¢"å›¾ç‰‡å‹ç¼©"ï¼Œåº”åŒ…å«æ‰¹é‡å‹ç¼©å·¥å…·ã€å›¾ç‰‡æ ¼å¼è½¬æ¢ç­‰
                    - å¦‚æœæœç´¢"ä»£ç æ ¼å¼åŒ–"ï¼Œåº”åŒ…å«ä»£ç ç¾åŒ–å·¥å…·ã€è¯­æ³•æ£€æŸ¥å™¨ã€ä»£ç é£æ ¼ç»Ÿä¸€ç­‰
                    - å¦‚æœæœç´¢"æ–‡æœ¬ç¿»è¯‘"ï¼Œåº”åŒ…å«æœºå™¨ç¿»è¯‘APIã€å¤šè¯­è¨€ç¿»è¯‘å·¥å…·ã€ç¦»çº¿ç¿»è¯‘è½¯ä»¶ç­‰
                    - å¦‚æœæœç´¢"æ•°æ®å¯è§†åŒ–"ï¼Œåº”åŒ…å«å›¾è¡¨ç”Ÿæˆå·¥å…·ã€æ•°æ®åˆ†æåº“ã€äº¤äº’å¼å›¾è¡¨ç­‰
                    - å¦‚æœæœç´¢"ç½‘ç»œçˆ¬è™«"ï¼Œåº”åŒ…å«æ•°æ®é‡‡é›†æ¡†æ¶ã€åçˆ¬è™«ç»•è¿‡ã€æ•°æ®è§£æå·¥å…·ç­‰
                    - å¦‚æœæœç´¢"APIæµ‹è¯•"ï¼Œåº”åŒ…å«æ¥å£æµ‹è¯•å·¥å…·ã€æ€§èƒ½ç›‘æ§ã€è‡ªåŠ¨åŒ–æµ‹è¯•æ¡†æ¶ç­‰
                6. æ‰€æœ‰å†…å®¹ä¸»è¦ä½¿ç”¨è‹±æ–‡è¡¨è¾¾ï¼Œå¹¶å¯¹éƒ¨åˆ†å…³é”®è¯æ·»åŠ é¢å¤–çš„ä¸­æ–‡è¡¨ç¤º
                7. è¿”å›å†…å®¹ä¸è¦ä½¿ç”¨ä»»ä½• markdown æ ¼å¼ ä»¥åŠä»»ä½•ç‰¹æ®Šå­—ç¬¦
            """),
            "zh2en": dedent("""
                ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­è¯‘è‹±ç¿»è¯‘ä¸“å®¶ï¼Œå°¤å…¶æ“…é•¿å­¦æœ¯è®ºæ–‡çš„ç¿»è¯‘å·¥ä½œã€‚
                è¯·å°†ç”¨æˆ·æä¾›çš„ä¸­æ–‡å†…å®¹ç¿»è¯‘æˆåœ°é“ã€ä¸“ä¸šçš„è‹±æ–‡ã€‚

                é‡è¦æç¤ºï¼š
                1. ä½¿ç”¨å­¦æœ¯è®ºæ–‡å¸¸ç”¨çš„è¡¨è¾¾æ–¹å¼å’Œæœ¯è¯­
                2. ä¿æŒä¸“ä¸šã€æ­£å¼çš„è¯­æ°”
                3. ç¡®ä¿è¯‘æ–‡çš„å‡†ç¡®æ€§å’Œæµç•…æ€§
                4. å¯¹ä¸“ä¸šæœ¯è¯­è¿›è¡Œå‡†ç¡®ç¿»è¯‘
                5. éµå¾ªè‹±æ–‡å­¦æœ¯å†™ä½œçš„è¯­æ³•è§„èŒƒ
                6. ä¿æŒåŸæ–‡çš„é€»è¾‘ç»“æ„
                7. é€‚å½“ä½¿ç”¨å­¦æœ¯è®ºæ–‡å¸¸è§çš„è¿‡æ¸¡è¯å’Œè¿æ¥è¯
                8. å¦‚é‡åˆ°æ¨¡ç³Šçš„è¡¨è¾¾ï¼Œé€‰æ‹©æœ€ç¬¦åˆå­¦æœ¯ä¸Šä¸‹æ–‡çš„ç¿»è¯‘
                9. é¿å…ä½¿ç”¨å£è¯­åŒ–æˆ–éæ­£å¼çš„è¡¨è¾¾
                10. æ³¨æ„æ—¶æ€å’Œè¯­æ€çš„å‡†ç¡®ä½¿ç”¨
            """),
            "github_score": dedent("""
                ä½ æ˜¯ä¸€ä¸ªè¯­ä¹‰åŒ¹é…è¯„åˆ†ä¸“å®¶ï¼Œæ“…é•¿æ ¹æ®ç”¨æˆ·éœ€æ±‚å’Œä»“åº“æè¿°è¿›è¡Œè¯­ä¹‰åŒ¹é…åº¦è¯„åˆ†ã€‚
                ç”¨æˆ·ä¼šè¾“å…¥ä¸¤éƒ¨åˆ†å†…å®¹:
                1. ç”¨æˆ·çš„å…·ä½“éœ€æ±‚æè¿°
                2. å¤šä¸ªä»“åº“çš„æè¿°åˆ—è¡¨(ä»¥1,2,3ç­‰æ•°å­—å¼€å¤´)
                
                è¯·ä½ ä»”ç»†åˆ†æç”¨æˆ·éœ€æ±‚ï¼Œå¹¶å¯¹æ¯ä¸ªä»“åº“è¿›è¡Œè¯„åˆ†ã€‚
                ç¡®ä¿è¿”å›ä¸€ä¸ªå¯ä»¥ç›´æ¥è¢« json åº“è§£æçš„å“åº”ï¼ŒåŒ…å«ä»¥ä¸‹å†…å®¹ï¼š
                {
                    "indices": [ä»“åº“ç¼–å·åˆ—è¡¨ï¼ŒæŒ‰åˆ†æ•°ä»é«˜åˆ°ä½],
                    "scores": [ç¼–å·å¯¹åº”çš„åŒ¹é…åº¦è¯„åˆ†åˆ—è¡¨ï¼Œ0-100çš„æ•´æ•°ï¼Œè¡¨ç¤ºåŒ¹é…ç¨‹åº¦]
                }
                
                é‡è¦æç¤ºï¼š
                1. è¯„åˆ†èŒƒå›´ä¸º0-100çš„æ•´æ•°ï¼Œé«˜äº60åˆ†è¡¨ç¤ºå…·æœ‰æ˜æ˜¾ç›¸å…³æ€§
                2. è¯„åˆ†è¦å®¢è§‚åæ˜ ä»“åº“ä¸éœ€æ±‚çš„å¥‘åˆåº¦
                3. åªè¿”å›è¯„åˆ†å¤§äº 60 çš„ä»“åº“
                4. è¿”å›å†…å®¹ä¸è¦ä½¿ç”¨ä»»ä½• markdown æ ¼å¼ ä»¥åŠä»»ä½•ç‰¹æ®Šå­—ç¬¦
            """)
        }
        return prompt_map[mode]

    def get_llm(self, model="gpt-4o-mini"):
        '''
        params:
            model: str, æ¨¡å‹åç§° ["gpt-4o-mini", "gpt-4o", "o1-mini", "gemini-1.5-flash-002"]
        '''
        llm = ChatOpenAI(
            model=model,
            base_url=self.base_url,
            api_key=self.api_key,
        )
        print(f"Init model {model} successfully!")
        return llm
    
    def ask_question(self, question, system_prompt=None):
        # 1. è·å–ç³»ç»Ÿæç¤º
        if system_prompt is None:
            system_prompt = self.get_system_prompt()
        
        # 2. ç”ŸæˆèŠå¤©æç¤º
        prompt = self.chat_prompt.format(input=question, system_prompt=system_prompt)
        config = {
            "configurable": {"response_format": {"type": "json_object"}}
        }
        
        # 3. è°ƒç”¨ LLM è¿›è¡Œå›ç­”
        for _ in range(10):
            try:
                response = self.llm.invoke(prompt, config=config)
                response.content = self.clean_json(response.content)
                return response
            except Exception as e:
                print(e)
                time.sleep(10)
                continue
        print(f"Failed to call llm for prompt: {prompt[0:10]}")
        return None
    
    async def ask_questions_parallel(self, questions, system_prompt=None):
        # 1. è·å–ç³»ç»Ÿæç¤º
        if system_prompt is None:
            system_prompt = self.get_system_prompt()

        # 2. å®šä¹‰å¼‚æ­¥å‡½æ•°
        async def call_llm(prompt):
            for _ in range(10):
                try:
                    response = await self.llm.ainvoke(prompt)
                    response.content = self.clean_json(response.content)
                    return response
                except Exception as e:
                    print(e)
                    await asyncio.sleep(10)
                    continue
            print(f"Failed to call llm for prompt: {prompt[0:10]}")
            return None

        # 3. æ„å»º prompt
        prompts = [self.chat_prompt.format(input=question, system_prompt=system_prompt) for question in questions]

        # 4. å¼‚æ­¥è°ƒç”¨
        tasks = [call_llm(prompt) for prompt in prompts]
        results = await asyncio.gather(*tasks)

        return results

class RepoSearch:
    def __init__(self):
        db_path = os.path.join(root_path, "database", "init")
        embeddings = OpenAIEmbeddings(api_key=os.getenv("OPENAI_API_KEY"), 
                                      base_url=os.getenv("OPENAI_BASE_URL"),
                                      model="text-embedding-3-small")
        
        assert os.path.exists(db_path), f"Database not found: {db_path}"
        self.vector_db = FAISS.load_local(db_path, embeddings, 
                                          index_name="init",
                                          allow_dangerous_deserialization=True)
        
    def search(self, query, k=10):
        '''
            name + description + html_url + topics
        '''
        results = self.vector_db.similarity_search(query + " technology", k=k)

        simple_str = ""
        simple_list = []
        for i, doc in enumerate(results):
            content = json.loads(doc.page_content)
            metadata = doc.metadata
            if content["description"] is None:
                content["description"] = ""
            # desc = content["description"] if len(content["description"]) < 300 else content["description"][:300] + "..."
            simple_str += f"\t**{i+1}. {content['name']}** || {content['description']}\n" # ç”¨äºå¤§æ¨¡å‹åŒ¹é…
            simple_list.append({
                "name": content["name"],
                "description": content["description"],
                **metadata,  # è§£åŒ…æ‰€æœ‰ metadata å­—æ®µ
            })

        return simple_str, simple_list

def main():
    search = RepoSearch()
    llm = OurLLM(model="gpt-4o")

    def respond(
        prompt: str,
        history,
        is_llm_filter: bool = False,
        is_keyword_expand: bool = False,
        match_num: int = 40
    ):
        # 1. åˆå§‹åŒ–å†å²è®°å½•
        if not history:
            history = [{"role": "system", "content": "You are a friendly chatbot"}]
        history.append({"role": "user", "content": prompt})
        response = {"role": "assistant", "content": ""}
        yield history

        # 2. æ‰©å±•ç”¨æˆ·é—®é¢˜å…³é”®è¯
        if is_keyword_expand:
            response["content"] = "å¼€å§‹æ‰©å±•å…³é”®è¯..."
            yield history + [response]

            query = llm.ask_question(prompt, system_prompt=llm.get_system_prompt("keyword_expand")).content
            prompt = ", ".join(json.loads(query)["keywords"])

        # 3. è¯­ä¹‰å‘é‡åŒ¹é…
        response["content"] = "å¼€å§‹è¯­ä¹‰å‘é‡åŒ¹é…..."
        yield history + [response]
        match_str, simple_list = search.search(prompt, match_num)

        # 4. é€šè¿‡ LLM è¯„åˆ†å¾—åˆ°æœ€åŒ¹é…çš„ä»“åº“ç´¢å¼•
        if not is_llm_filter:
            simple_strs = [f"\t**{i+1}. {repo['name']}** [âœ¨ {repo['star_count'] // 1000}k] || **Description:** {repo['description']} || **Url:** {repo['html_url']} \n" for i, repo in enumerate(simple_list)]
            response["content"] = "".join(simple_strs)
            yield history + [response]
        else:
            response["content"] = "å¼€å§‹é€šè¿‡ LLM è¯„åˆ†å¾—åˆ°æœ€åŒ¹é…çš„ä»“åº“..."
            yield history + [response]

            query = ' ## ç”¨æˆ·éœ€è¦çš„ä»“åº“å†…å®¹ï¼š' + prompt + '\n ## æœç´¢ç»“æœåˆ—è¡¨ï¼š' + match_str
            out = llm.ask_question(query, system_prompt=llm.get_system_prompt("github_score")).content
            matched_index = json.loads(out)["indices"]

            # 5. é€šè¿‡ç´¢å¼•å¾—åˆ°æœ€åŒ¹é…çš„ä»“åº“
            result = [simple_list[idx-1] for idx in matched_index]
            simple_strs = [f"\t**{i+1}. {repo['name']}** [âœ¨ {repo['star_count'] // 1000}k] || **Description:** {repo['description']} || **Url:** {repo['html_url']} \n" for i, repo in enumerate(result)]
            response["content"] = "".join(simple_strs)
            yield history + [response]

    with gr.Blocks() as demo:
        gr.Markdown("## Github semantic search (åŸºäºè¯­ä¹‰çš„ github ä»“åº“æœç´¢) ğŸŒ")
        
        with gr.Row():
            with gr.Column(scale=1):
                # æ·»åŠ æ§åˆ¶å‚æ•°
                llm_filter = gr.Checkbox(
                    label="ä½¿ç”¨LLMè¿‡æ»¤ç»“æœ",
                    value=False,
                    info="æ˜¯å¦ä½¿ç”¨ LLM å¯¹æœç´¢ç»“æœè¿›è¡ŒäºŒæ¬¡è¿‡æ»¤"
                )
                keyword_expand = gr.Checkbox(
                    label="æ‰©å±•å…³é”®è¯æœç´¢",
                    value=False,
                    info="æ˜¯å¦ä½¿ç”¨ LLM æ‰©å±•æœç´¢å…³é”®è¯"
                )
                match_number = gr.Slider(
                    minimum=10,
                    maximum=100,
                    value=40,
                    step=10,
                    label="è¯­ä¹‰åŒ¹é…æ•°é‡",
                    info="è¿›è¡Œè¯­ä¹‰åŒ¹é…åè¿”å›çš„ä»“åº“æ•°é‡ï¼Œè‹¥ä½¿ç”¨ LLM è¿‡æ»¤ï¼Œå»ºè®®é€‚å½“å¢åŠ æ•°é‡"
                )
            
            with gr.Column(scale=3):
                chatbot = gr.Chatbot(
                    label="Agent",
                    type="messages",
                    avatar_images=(None, "https://img1.baidu.com/it/u=2193901176,1740242983&fm=253&fmt=auto&app=138&f=JPEG?w=500&h=500"),
                    height="65vh"
                )
                prompt = gr.Textbox(max_lines=2, label="Chat Message")
                
        # æ›´æ–°submitè°ƒç”¨ï¼ŒåŒ…å«æ–°çš„å‚æ•°
        prompt.submit(
            respond, 
            [prompt, chatbot, llm_filter, keyword_expand, match_number], 
            [chatbot]
        )
        prompt.submit(lambda: "", None, [prompt])

    demo.launch(share=False)


if __name__ == "__main__":
    main()